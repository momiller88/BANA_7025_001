---
output: 
  rmarkdown::html_document:
    theme: yeti
runtime: shiny
---
## Pizza Project Final {.tabset .tabset-fade .tabset-pills}
**Authors: Sidney Schaeper & Morgan Miller **

### Introduction


![](https://i.ibb.co/tPSCbDf/pizza-1.jpg)


#### **Do You Love Pizza?**

We would say most people love pizza. Also, most people want to try a new pizza when they travel or move somewhere new. This is the problem we are hoping to help answer through our analysis. Yelp and other food review sites are great for finding new food locations to eat at, but what they lack is the ability to tell you upfront what is the actual price range of the restaurant. They only provide you a 5 scale rating of the price range, and this can be interpreted very differently based on the person. Our goal is to show you the top 5 pizza places by price range, zip, and type of rating. Our analysis will provide you with a more concrete number of the price range of a pizza place. We are planning to create a table of the the top rated pizza places by zip code, price range, and type of review. We are planning to make this interactive, so the user can select a zip code, price range, and type of review. After this selection, the table would automatically fill with the top 5 best pizza places based on their selections. We are planning to complete this analysis through the use of the shiny, because shiny creates interactive tables. Also, we plan to combine the barstool and datafiniti datasets, so we can have reviews and price ranges. Also, the table will contain the name, address, price range, and rating of the pizza place. This table will help the user find the best pizza place they can afford to eat at whenever they are traveling or want to try a new pizza place.

The next question we are hoping to answer is what leads to a pizza place attaining a high rating. As an owner of a restaurant, you want to know what will possibly increase your rating as a pizza place, or you might want to know what you need to maintain to keep a high rating. This is the information we are hoping to provide to pizza place owners. We plan to answer this question by combining all three datasets. Also, We plan to answer this question through analysis of the correlation between variables, and performing regression analysis. We hope that these two analytical techniques will allow us to determine the relationship between certain varibles and the rating of pizza place. We believe that knowing these relationships will help an owner have a better awareness of what they need to focus on at their restaurants. Also, it will hopefully help them increase the amount of business they receive.

### Packages Required

#### **Packages**
The packages that are needed for this evaluation are the tidyverse, readr, dplyr, ggplot2, and DT. The tidyverse package contains multiple other packages that are useful with data manipulation and evaluation. You can find more information on the tidyverse package by clicking [here](https://tidyverse.tidyverse.org/). The readr package helps us import the datasets for this project. You can find more information on the readr package by clicking [here](https://cran.r-project.org/web/packages/readr/readr.pdf). The tidyr package helps us reorganize the datasets. You can find more information on the tidyr package by clicking [here](https://www.rdocumentation.org/packages/tidyr/versions/0.8.3). The ggplot2 package helps us create visualizations for these datasets. You can find more information on the ggplot2 package by clicking [here](https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf). The DT package helps us create data tables from the R output. You can find more information on the DT package by clicking [here](https://rstudio.github.io/DT/). The shiny package helps us create interactive data visualizations. You can find more information on the shiny package by clicking [here](https://cran.r-project.org/web/packages/shiny/index.html). The pastecs package helps us summarize the numeric values from the datasets. You can find more information on the pastecs package by clicking [here](https://www.rdocumentation.org/packages/pastecs/versions/1.3.21). The tibble package helps us add row.names to the character variable summary. You can find more information on the tibble package by clicking [here](https://tibble.tidyverse.org/reference/tibble-package.html). The first section of code allows you to install the packages. The second section of code allows you to load the packages into R. You need to run both of these sections of code in order to get these packages into R. 

##### **Code to Install Packages**
```{r eval=FALSE}
#Install packages necessary for this project#
install.packages("tidyverse")
install.packages("readr")
install.packages("tidyr")
install.packages("ggplot2")
install.packages("DT")
install.packages("shiny")
install.packeges("pastecs")
install.packages("tibble")
install.packages("mapdata")
```

##### **Code to Find Packages in the Library**
```{r results = "hide", warning = FALSE, message = FALSE}
#Load the installed packages into R#
library(tidyverse)
library(readr)
library(tidyr)
library(ggplot2)
library(DT)
library(shiny)
library(pastecs)
library(tibble)
library(mapdata)
```

### Source Data and Importing Dataset

#### **Source Data**
The source data link we were provided for these datasets was found on github at the following [link](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-10-01). The three datasets that were on this github were called pizza_barastool, pizza_datafiniti, and pizza_jared. These datasets were uploaded to this github on September 30, 2019 by Thomas Mock. 

The pizza barstool data was created by Tyler Richards on May 11, 2019. Tyler Richards orginial use for the data was to find the best pizza places in New York, because he thought other search tools weren't that clean. Also, he had just moved to New York. Tyler Richards got his data from Barstool's Dave Portnoy's Instagram and One Bite. The way Tyler pulled in this data was by reverse engineering the API from charles. Charles is an application that held Dave's review score. Also, he pulled in the longitude and latitude using Google maps reverse geocoder. The variables that he included in his dataset are: 

* _distance 
* address1 
* address2categories
* city
* country
* createdAt
* deleted
* featuredMedia.__t
* featuredMedia.__v
* featuredMedia._id
* featuredMedia.assetId
* featuredMedia.createdAt
* featuredMedia.deleted
* featuredMedia.id
* featuredMedia.metadata.aspectRatio
* featuredMedia.metadata.duration
* featuredMedia.metadata.frameRate
* featuredMedia.metadata.resolution
* featuredMedia.modifiedAt
* featuredMedia.playbackId
* featuredMedia.reviewId
* featuredMedia.sourceUrl
* featuredMedia.status
* featuredMedia.streams.hls
* featuredMedia.streams.mp4
* featuredMedia.thumbnails.large
* featuredMedia.thumbnails.medium
* featuredMedia.thumbnails.small
* featuredMedia.type
* featuredMedia.user
* id
* imageUrl
* loc.coordinates
* loc.type
* modifiedAt
* name
* openHours
* orderProvider
* orderProvider.checkoutUrl
* orderProvider.internalId
* orderProvider.logo
* orderProvider.name
* phoneNumber
* photos
* placeId
* priceLevel
* providerRating
* providerReviewCount
* providerTransactions
* providerUrl
* refreshDate
* reviewStats.all.averageScore
* reviewStats.all.count
* reviewStats.all.totalScore
* reviewStats.community.averageScore
* reviewStats.community.count
* reviewStats.community.totalScore'
* reviewStats.critic.averageScore
* reviewStats.critic.count
* reviewStats.critic.totalScore
* reviewStats.dave.averageScore
* reviewStats.dave.count
* reviewStats.dave.totalScore
* slug
* state
* timeZone
* type
* zip
* address_total
* accuracy
* formatted_address
* google_place_id
* input_string
* latitude
* longitude
* number_of_results
* postcode
* response.error_message
* response.results
* response.status
* status
* type.1
* string_concat
* dave_score_int
* review_number

These variables were not the same as github provided to us for our source data. The variables that were provided to us from the github are below in the data dictionary. You can see Tyler Richards analysis at this [website](https://towardsdatascience.com/adventures-in-barstools-pizza-data-9b8ae6bb6cd1) and you can see his dataset at this [website](https://github.com/tylerjrichards/Barstool_Pizza). 

The next dataset is the datafinit dataset. This dataset was provided by Datafiniti's Business Database. We pulled our source data from the following [github](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-10-01). The github that we pulled our source data from got their data from kaggle at the following [link](https://www.kaggle.com/datafiniti/pizza-restaurants-and-the-pizza-they-sell#Datafiniti_Pizza_Restaurants_and_the_Pizza_They_Sell_May19.csv). Datafiniti uploaded this dataset on May 29, 2019 from the Datafiniti Business Database. You find information about datafiniti at the following [link](https://datafiniti.co/). The variables that are included in this dataset on kaggle are:

* id
* address
* categories
* city
* country
* keys
* latitude
* longitude
* menuPageURL
* menus.amountMax
* menus.amountMin
* menus.currency
* menus.dateSeen
* menus.description
* menus.name
* name
* postalCode
* priceRangeCurrency
* priceRangeMin
* priceRangeMax
* province

The variables that were found in the dataset on kaggle are not the same as what we found in github, The variables that were found in the source data on github are below in the data dictionary. 

The final dataset is the jared dataset. This dataset was pulled from [Jared Lander's twitter](https://twitter.com/jaredlander/status/1178122846419193858?s=20). He provided Thomas the [json](https://jaredlander.com/data/PizzaPollData.php) from his database on various pizza ratings he collected. He provided this data on September 28, 2019. His database contained the following variables: 

* PollaQid
* Answer
* Votes
* pollq_id
* question
* place
* time 
* totalvotes
* percent

The variables that Jared had orginally populated were not all included on github. The variables used on the [github page](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-10-01) are below. 

#### **Original Data Dictionary from Github**
##### Pizza Jared Variables
Variable | Class | Description
---------|-------|------------
polla_qid | integer | quiz id
answer | character | answer (likert scale)
votes | integer | number of votes for that question/answer combo
pollq_id | integer | poll question id
question | character | pizza place
time | integer | time of quiz
total_votes | integer | total number of votes for that pizza place
percent | double | vote percent of total for that pizza place

##### **Pizza Barstool Variables**
Variable | Class | Description
---------|-------|------------
name | character | pizza place name
address1 | character | pizza place address
city | character | city
zip | double | zip
country | character | country
latitude | double | latitude
longitude | double | longitude 
price_level | double | price rating (0=cheap and 3=expensive)
provider_rating | double | provider review score
provider_review_count | double | provider review count 
review_stats_all_average_score | double | average score
review_stats_all_count | double | count of all reviews
review_stas_all_total_score | double | review total score
review_stats_community_average_score | double | community average score
review_stats_community_count | double | community review count
review_stats_community_total_score | double | community review total score
review_stats_critic_average_score | double | critic average score
review_stats_critic_count | double | critic review count
review_stats_critic_total_score | double | critic total score
review_stats_dave_average_score | double | Dave (Barstool) average score
review_stats_dave_count | double | Dave review count
review_stats_dave_total_score | double | Dave total score

##### **Pizza Datafiniti Variables**
Variable | Class | Description
---------|-------|------------
name | character | pizza place
address | character | address
city | character | city 
country | character | country
province | character | state
latitude | double | latitude 
longitude | double |longitude
categories | character | restaurant category
price_range_min | double | price range min
price_range_max | double | price range max

#### **Setting the Working Directory and Reading the Datasets Into R**
The following is the code to read the datasets in the R. You first need to set the working directory to wherever the datasets are located. Next, you need to read the file into R. Before you set the working directory and the read datasets into R, you need to install the packages mentioned in the prior tab. Also, you need to find them in the library. The following code shows you how to do these steps. 

```{r eval = TRUE, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
#Set the working directory#
knitr::opts_knit$set(root.dir = getwd())

#Read the data sets into R#
barstool <- read_csv("pizza_barstool.csv")
datafiniti <- read_csv("pizza_datafiniti.csv")
jared <- read_csv("pizza_jared.csv")
```

The first thing you should do is to make sure you have read in the dataset. Below is a snippet of each of the datasets. 

```{r}
#See a snippet of the barstool dataset#
datatable(head(barstool, 10))

#See a snippet of the datafiniti dataset#
datatable(head(datafiniti, 10))

#See a snippet of the jared dataset#
datatable(head(jared, 10))
```

### Data Cleaning
#### **Evaluate Missing Values**
The first item we wanted to evaluate was the missing items in the datasets. The website for the source data didn't mention missing items or how they were recorded. We decided that we needed to do this on our own through R. The code below allows us to see the percentage of missing items for each of the datasets. 

```{r}
#Evaluate the percentage of missing items for barstool#
(colSums(is.na(barstool)) / 463) * 100

#Evaluate the percentage of missing items for datafiniti#
(colSums(is.na(datafiniti)) / 10000) * 100

#Evaluate the percentage of missing items for jared#
(colSums(is.na(jared)) / 375) * 100
```

For the barstool dataset, longitude and latitude have missing items. The percentage of missing items for longitude and latitude is 0.43%. For the datafiniti dataset, there were no missing items for the dataset. For the jared dataset, percent is the only variable to have missing items. The percentage of missing items for percent is 1.33%. Once we realized there were missing items, we looked through the dataset to see how the missing values were recorded. We saw that all the missing items were recorded with an NA. After evaluating the missing items, we decided to drop the rows that were missing, because there was such a small percentage of missing items. Below is the code to remove these missing items. 

```{r}
#Removes the missing values for each of the datasets#
barstool_miss <- na.omit(barstool)
datafiniti_miss <- na.omit(datafiniti)
jared_miss <- na.omit(jared)
```

#### **Remove Deduplicates and Drop Unneccessary Columns**
For the datafiniti dataset, we noticed there were a lot of duplicate values. After removing the duplicates of the dataset, we went from 10,000 records to 2,285 unique records. Both, the barstool and jared, datasets didn't contain true duplicates. Additionally, we decided to remove the categories column of the datafiniti dataset. We did this because it didn't add significant value to our end evaluation. All the records contained the category of "Pizza Place", so without additional descriptions of each of the other possible categories we decided it wasn't needed. 

```{r}
#Remove duplicates#
nrow(datafiniti)
datafiniti_dup_removed <- datafiniti[!duplicated(datafiniti, nmax = 1), ]
nrow(datafiniti_dup_removed)
```

```{r}
#Remove 'categories' column#
datafiniti_dup_removed <- datafiniti_dup_removed[-c(8)]
names(datafiniti_dup_removed)
```

#### **Reformat Dataset**
Upon an innitial look at the jared dataset it appears that each poll taken was then recorded based on the reponses that were given. This duplicated the poll_id for each instance 5 times. In order to fix this we need to flatten out the responses and make them their own columns and correlate the listed response values with the appropriate response column. Additionally, we would then have to ensure that the totals and calculated percentages were flattened out as well and associated with the correct poll. In doing this, we also noticed that some of the some of the resturants could have been recorded across multiple polls. This could have been a poll on two seperate locations with the same name. However, without location information associated with each poll, we could only assume that the same resturant was polled more than once. We decided to combine the values of the duplicated polls and recalculate the associated percentages and vote totals. In order to do this, you will need to use the code below.

```{r}
#'Jared' Attributes#
attributes(jared)

#Table to see the duplicated results for each attribute#
table(jared$polla_qid)
table(jared$answer)
table(jared$votes)
table(jared$pollq_id)
table(jared$question)
table(jared$place) 
table(jared$time)
table(jared$total_votes)
table(jared$percent)
```

Here we are creating a separate dataframe to flatten out each response and display the count of votes under the respective resoponse. 

```{r}
#Select and spread responses count#
df1 <- jared %>% 
  select(polla_qid, answer, votes) %>%
  spread(answer, votes, fill = 0)
datatable(head(df1))
```

Here we are doing the same for the percentage results for each of the responses and renaming the percentage columns appropriately.

```{r}
#Select and spread responses percent#
df2 <- jared %>%
  select(polla_qid, answer, percent) %>%
  spread(answer, percent, fill = 0)
names(df2) <- c("polla_qid", "Pct_Average", "Pct_Excellent", "Pct_Fair", "Pct_Good", "Pct_Never_Again", "Pct_Poor")
datatable(head(df2))
```

We are joining the two new dataframes on 'polla_qid' into a new dataframe named 'dfm'.

```{r}
#Merge df1 and df2
dfm <- merge(x = df1, y = df2, by = "polla_qid", all.x = TRUE)
datatable(head(dfm))
```

Now that we have a single dataframe ('dfm') that contains all of the flattened responses and associated percentages, we need to remerge the responses with their associated question, place, time, and total votes. We can do that by deduplacting a dataframe generated by selecting only those columns with the 'pollq_id'. 'pollq_id' will be used to merge results with the appropriate question. 

```{r}
#Select remaining columns and dedup question by poll#
df3 <- jared %>%
  select(pollq_id, question, place, time, total_votes)
df3 <- df3[!duplicated(df3, nmax = 1,), ]
datatable(head(df3))
#Merge votes df and question/location df#
jared_clean <- merge(x = df3, y = dfm, by.x = "pollq_id", by.y = "polla_qid", all.x = TRUE)  
jared_clean <- select(jared_clean, 
                      place, 
                      question, 
                      total_votes, 
                      `Never Again`, 
                      Poor, Fair, 
                      Average, 
                      Good, 
                      Excellent, 
                      Pct_Never_Again, 
                      Pct_Poor, 
                      Pct_Fair, 
                      Pct_Average, 
                      Pct_Good, 
                      Pct_Excellent)
head(jared_clean)
```

Now that our dataset is reformatted, we can combine the duplicate values and resummarize the voting statistics. We start by identifying the records that still contain duplicate values. We do this by creating a dataframe, 'jared_dups', that counts the frequency of each place in the dataset. Next, we remove all names from that list that only occur once. 

```{r}
#Identifying columns that have duplicated names in jared_clean#
jared_dups <- data.frame(table(jared_clean$place))

#Shorten list to only dup records#
jared_dups[jared_dups$Freq > 1, ]
jared_dups <- arrange(jared_clean[jared_clean$place %in% jared_dups$Var1[jared_dups$Freq > 1], ], desc(place))
datatable(head(jared_dups))
```

Now that we have a complete list of only the duplicated records, we can sum the duplicated values voting stats. We do this by grouping the records by place and question and using the summarise() function we total the sum of each of the vote count columns. We then save this to the dataframe 'jared_dups_combined'. In this dataframe, we dropped the existing percentage values, because they will need to be recalculated based on our new totals. 

```{r}
#Sum duplicate records votes#
jared_dups_combined <- 
  jared_dups %>%
    group_by(place, question) %>%
    summarise(total_votes = sum(total_votes), 
              `Never Again` = sum(`Never Again`),
              Poor = sum(Poor),
              Fair = sum(Fair),
              Average = sum(Average),
              Good = sum(Good),
              Excellent = sum(Excellent))
datatable(head(jared_dups_combined))

```

This is where we do our recalculations of the percentages. Using the 'jared_dups_combined' dataset, we use the mutate() function to recreate our percentage columns calculating the percentage of each response category. We had some issues editing the column name of the 'Never Again' column when it was saved as a tibble becuase it was named 'Never.Again'. So, we resaved it as a dataframe and renamed the column correctly so we could remerge it with the original dataset. 

```{r, warning = FALSE}
#Recalculate percents columns#
jared_dups_pct <- 
  jared_dups_combined %>%
  mutate( Pct_Never_Again = `Never Again` / total_votes, 
          Pct_Poor = Poor / total_votes,
          Pct_Fair = Fair / total_votes,
          Pct_Average = Average / total_votes,
          Pct_Good = Good / total_votes,
          Pct_Excellent = Excellent / total_votes)
jared_dups_pct <- as_data_frame(jared_dups_pct)
colnames(jared_dups_pct)[4] <- "Never Again"
datatable(head(jared_dups_pct))

```

Here we are removing all the records we had previously pulled out and combined. We do this by saving the list of places that were duplicated as 'jd_dup_names'. We then save our dataset as 'jc_wo_dupnames' by excluding records that have the same name as any of those in our saved list. We then save our final cleaned version of the jared dataset by using the rbind() function to merge our 'jc_wo_dupnames' dataset and 'jared_dups_pct' dataset and we save that as 'jared2'. 

```{r}
#Remove duplicated records from jared_clean#
jd_dup_names <- jared_dups_pct$place

jc_wo_dupnames <- jared_clean[ ! jared_clean$place %in% jd_dup_names, ]

datatable(head(jc_wo_dupnames))
datatable(head(jared_dups_pct))
#Remerge the two datasets to coplete the final cleaned version of the jared dataset#
jared2 <- rbind(jc_wo_dupnames, jared_dups_pct)
```


For our final dataset 'jared2' we chose to exclude the ID columns and the time column. We did this becuase after all the curation of the data they no longer provided additional value.


#### **Evaluate the Column Names and Data Types** 
The next item we wanted to evaluate was the column names and the data types for each column in the barstool dataset. First, you should type in the following code into R. Finally, you should run the code. The head code allows us to see a piece of the data. The names code allows us to see the name of the variables within the dataset. The tibble code allows us to see the data types for each of the variables.

```{r}
#See the variable names of the barstool data set#
names(barstool_miss)

#See the data types for the barstool variables#
tibble(barstool_miss)
```

After evaluating the outputs of the names and data types, there are multiple changes we believe that need to be happen. The first item we believe that needs to be changed is the name for address1. We decided to change the name for address1 to address, because it seemed more consistent with the other variable names. We believe the rest of the names of the variables are fine, and we don't need to change their names. You should the run the code below to change the name for address1.  

```{r}
#Change the name of address 1 to address for the barstool data set#
names(barstool_miss)[2] <- "address"
```

The next item we wanted to change was the data types for zip, price_level, provider_review_count, review_stats_all_count, review_stats_community_count, review_stats_critic_count, and review_stats_dave_count. We decided to change zip from a double to a character, because we wanted to maintain the zeros in front of some of the zip codes. We decided to change price_level, provider_review_count, review_stats_all_count, review_stats_community_count, review_stats_critic_count, and review_stats_dave_count from a double to integer, because there should be no decimals with these numbers. Below is the code to change these data types. 

```{r}
#Change the data type of the items mentioned above#
barstool_data_type <- transform(barstool_miss, zip = as.character(zip), price_level = as.integer(price_level), provider_review_count = as.integer(provider_review_count), review_stats_all_count = as.integer(review_stats_all_count), review_stats_community_count = as.integer(review_stats_community_count), review_stats_critic_count = as.integer(review_stats_critic_count), review_stats_dave_count = as.integer(review_stats_dave_count))

#Check that the data types were appropriately changed#
tibble(barstool_data_type)
```

#### **Evaluate the Values for the Variables**
The next item we wanted to evaluate was the values for the variables in the barstool dataset. The first variable we evaluated was the name in the barstool dataset. The first line of code creates a table with all the values for the name variable and how many times they were repeated. We only wanted to look at the items that were listed more than the once. The second line of code allows us to do that. Run the code below to see the output. 

```{r}
#Creates a table of names and shows only the ones greater than 1#
barstool_names <- table(barstool_data_type$name)
barstool_names[barstool_names > 1]
```

After evaluating the output for names, we searched the internet for these places to confirm that they were the same chain restaurant. In order to search these restaurants, we needed to pull in the rest of the variables to help with our search. The code below allows us to see the records where the same name was written more than once. 

```{r}
#Allows you to see the duplicate name records#
datatable(barstool_data_type[barstool_data_type$name == "Bonanno's New York Pizzeria", ])
datatable(barstool_data_type[barstool_data_type$name == "Joe's Pizza",])
datatable(barstool_data_type[barstool_data_type$name == "Justino's Pizzeria",])
datatable(barstool_data_type[barstool_data_type$name == "Kiss My Slice",])
datatable(barstool_data_type[barstool_data_type$name == "Little Italy Pizza",])
datatable(barstool_data_type[barstool_data_type$name == "Lucali",])
datatable(barstool_data_type[barstool_data_type$name == "Mariella Pizza",])
datatable(barstool_data_type[barstool_data_type$name == "Patsy's Pizzeria",])
datatable(barstool_data_type[barstool_data_type$name == "Regina Pizzeria",])
datatable(barstool_data_type[barstool_data_type$name == "Steve's Pizza",])
```

After performing our search, we found that Joe's Pizza at 8th is not the same chain as Joe's Pizza at Broadway and Carmine. After discovering this information, we decided to change the name for Joe's Pizza at 8th to Joe's Pizza - 8th. Also, we discovered that the Little Italy records were not the same chain restaurant. We decided to change the first record of Little Italy Pizza to Little Italy Pizza - 45th, and we decided to change the second record of Little Italy Pizza to Little Italy Pizza - Fulton. Next, we discovered that the Lucali records were not the same restaurant chain. We decided to change the first record of Lucali to Lucali - Henry, and we decided to change the second record of Lucali to Lucali - Bay. Also, we found that the Mariella Pizza was not the same chain. We changed the first record to Mariella Pizza - Lexington, and we changed the second record to Mariella Pizza - 8th. Finally, we found that Patsy's Pizzeria records were not the same chain. We changed the first record to Patsy's Pizzeria - 1st, and we changed the second record to Patsy's Pizzeria - 2nd. The rest of the names that we found that had duplicate names were part of the same chain restaurant. The code below allows us to change the names of the items that were mentioned before.

```{r}
#Changes the values of the restaurants that aren't part of the same chain#
barstool_data_type[c(198, 238, 239, 248, 249, 265, 266, 310, 311),]$name <- c("Joe's Pizza - 8th","Little Italy Pizza - 45th", "Little Italy Pizza - Fulton","Lucali - Henry", "Lucali - Bay", "Mariella Pizza - Lexington", "Mariella Pizza - 8th", "Patsy's Pizzeria - 1st", "Patsy's Pizzeria - 2nd")
```

The second variable we evaluated was the address in the barstool dataset. The first line of code creates a table with all the values for the address variable and how many times they were repeated. We wanted to look at the items that were listed more than the once. The second line of code allows us to do that. The third line of code allows us to see addresses where they don't start off with a house number. Run the code below to see the output. 

```{r}
#Creates a table of addresses and shows only the ones greater than 1#
barstool_addresses <- table(barstool_data_type$address)
barstool_addresses[barstool_addresses > 1]

#Shows addresses without house numbers#
barstool_data_type[barstool_data_type$address > "989 1st Ave", 2]
```

After evaluating the output for addresses, we searched the internet for these places to confirm that they were at the same location. In order to search these restaurants, we need to pull in the rest of the variables to help with our search. The code below allows us to see the records where the same address was written more than once. 

```{r}
#Creates data table for the addresses we needed to search for#
datatable(barstool_data_type[barstool_data_type$address == "268 W 23rd St",])
datatable(barstool_data_type[barstool_data_type$address == "3131 Las Vegas Blvd S",])
datatable(barstool_data_type[barstool_data_type$address == "936 8th Ave",])
```

After performing our search, we found that the restaurants at 268 W 23rd St and 3131 Las Vegas Blvd S were both located at the same place. The restaurants at 936 8th Ave were the exact same restaurant. After evaluating both records, we decided to drop the first one, because there were more reviews in each category except provider_review_count. Below is the code to drop the first record of 936 8th Ave. 

```{r}
#Remove the duplicate address#
barstool_variable_name <- barstool_data_type[-c(250),]
```

Next, we wanted to change the address for the items that don't start with a house number, except for Broadway Between 42nd And 43rd St. We decided that it would be difficult to do any geo-location analysis without the address being correct. We decided not to change Broadway Between 42nd And 43rd St, because it showed up as the right address on google. Below is the code to change these addresses that needed to be changed. 

```{r}
#Change the values for the improper addresses#
barstool_variable_name[barstool_variable_name$address == "MGM Hotel & Casino Food Court", ]$address <- "3799 S Las Vegas Blvd"
barstool_variable_name[barstool_variable_name$address == "Home Depot Plz", ]$address <- "3039 NY-50"
barstool_variable_name[barstool_variable_name$address == "Pier 86 W 46th St 12th Ave", ]$address <- "Pier 86, W 46th St"
barstool_variable_name[barstool_variable_name$address == "The Metlife Building", ]$address <- "200 Park Ave"
```

The third variable we evaluated was the city in the barstool dataset. The line of code below creates a table with all the values for the city variable and how many times they were repeated. We wanted to evaluate if there were any inproper cities selected. 

```{r}
#Table of Cities#
table(barstool_variable_name$city)
```

After evaluating the results from the table, the cities we found to have improper values were DUMBO, Edina, New York, Mashantucket, Chilmark, and Orange. We found the proper value for these cities by searching the addresses. The code below allows us to fix this improper values. 

```{r}
#Change Improper Names of Cities#
barstool_variable_name[barstool_variable_name$city == "DUMBO", ]$city <- "Brooklyn"
barstool_variable_name[barstool_variable_name$city == "Edina", ]$city <- "Minneapolis"
barstool_variable_name[barstool_variable_name$city == "New York", ]$city <- "New York City"
barstool_variable_name[barstool_variable_name$city == "Mashantucket", ]$city <- "Ledyard"
barstool_variable_name[barstool_variable_name$city == "Chilmark", ]$city <- "Aquinnah"
barstool_variable_name[barstool_variable_name$city == "Orange", ]$city <- "City of Orange"
```

The fourth variable we evaluated was the zip in the barstool dataset. The line of code below creates a table with all the values for the zip variable and how many times they were repeated. We wanted to evaluate the items that didn't have five numbers for the zip code.

```{r}
#Zip Code Table#
barstool_zip <- barstool_variable_name[as.numeric(barstool_variable_name$zip) < 10000, 4]
barstool_zip
```

After evaluating the table and searching some of these zip codes, we found that the items that had only 4 digits for the zip code were missing a 0 in front. The zip code should always be 5 digits. The code below allows us to fix this problem.

```{r}
#Fixes the zip codes with only 4 digits, add a beginning 0#
zip_edit <- barstool_variable_name$zip
fixed_zip <- unlist(lapply(zip_edit, function(x){if(nchar(x)<5){paste0(0,x)}else{x}}))
barstool_variable_name$zip <- fixed_zip
```

Next, we evaluated the rest of the variables. We found them to be fine after looking at their tables. We noticed that there were outliers with the some of the numeric variables. We didn't want to remove any, because certain locations could receive more votes due to popularity. Also, the outliers were still reasonable numbers. If you would like to see these values, please run the code below. Also, you can see some of the ouliers in the histograms in the next tab.  

```{r eval = TRUE, results = "hide", fig.show = "hide", message = FALSE, warning = FALSE}
#Creates the tables for the other values in the dataset we didn't fix a value for#
table(barstool_variable_name$country)
table(barstool_variable_name$latitude)
table(barstool_variable_name$longitude)
table(barstool_variable_name$price_level)
table(barstool_variable_name$provider_rating)
table(barstool_variable_name$provider_review_count)
table(barstool_variable_name$review_stats_all_average_score)
table(barstool_variable_name$review_stats_all_count)
table(barstool_variable_name$review_stats_all_total_socre)
table(barstool_variable_name$review_stats_community_average_score)
table(barstool_variable_name$review_stats_community_count)
table(barstool_variable_name$review_stats_community_total_score)
table(barstool_variable_name$review_stats_critic_average_score)
table(barstool_variable_name$review_stats_critic_count)
table(barstool_variable_name$review_stats_dave_average_score)
```

### Final Cleaned Datasets

The code below allows us to see a snippet of the final cleaned dataset for barstool. 
```{r}
#Final cleaned dataset for barstool#
datatable(head(barstool_variable_name, 50))
```

The code below allows us to see a snippet of the final cleaned dataset for datafiniti. 

```{r}
#Final cleaned dataset for datafiniti#
datatable(head(datafiniti_dup_removed, 50))
```

The code below allos us to see a snippet of the final cleaned dataset for jared. 

```{r}
#Final cleaned dataset for jared#
datatable(head(jared2, 50))
```

### Summary of Variables

#### **Summary of Barstool Variables**
Next, we wanted to evaluate the summary of the barstool variables by character and numeric variables. The first table lets us see the summary of the character variables. We summarized the character variables by the variable name, data type, length, and number of unqiue variables. See below for the table.

```{r}
#Creates summary data table of the character variables for barstool#
sum_char_bar <- barstool_variable_name[ , c(1:5)]
fun_bar <- function(sum_char_bar){list(typeof(sum_char_bar), length(sum_char_bar), length(unique(sum_char_bar)))}
app_char_bar <- sapply(sum_char_bar, fun_bar)
row.names(app_char_bar) <- c("Data Type", "Length", "Number of Unique Values")
datatable(app_char_bar)
```

This table lets us see a summary of the numeric variables for the barstool dataset. We summarized this dataset by variable, number of values, number of zero values, number of missing values, minimum, maximum, range, sum, median, mean, standard error mean, confidence interval mean, variance, standard deviation, and coefficient variance. Also, we wanted to summarize the numeric variables through a histogram. We didn't include the longitude, latitude, and review_stats_dave_count. We didn't include the longitude and latitude, because there is too much variablity with these variables. Also, we didn't include the review_stats_dave_count, because it is the same value all the way through the dataset. See below for the table and histogram. Select the appropriate variable you would like to evaluate through a histogram. 

```{r}
#Creates summary data table for the numeric variables for barstool#
sum_num_bar <- stat.desc(barstool_variable_name[ , -c(1:5)])
datatable(round(sum_num_bar, 2))
```

```{r}
#Creates a new dataset for the items that we want evaluated by a histogram#
barstool_for_histogram <- barstool_variable_name[ , c(8:20,22)]
#Creates the interactive histogram#
shinyApp(
  ui = fluidPage(
    varSelectInput("variable", "Variable:", barstool_for_histogram),
    plotOutput("data" , height = 300, width = 700)
),
server = function(input, output) {
  output$data <- renderPlot({
    ggplot(barstool_for_histogram, aes(!!input$variable)) + geom_histogram()})
}
)
```

#### **Summary of Datafiniti Variables**
Now, we are going to evaluate the summary of the datafiniti variables by character and numeric variables. The first table lets us see the summary of the character variables. We summarized the character variables by the variable name, data type, length, and number of unqiue variables. See below for the table.

```{r}
#Creates summary data table of the character variables for datafiniti#
sum_char_dat <- datafiniti_dup_removed[ , c(1:5)]
fun_dat <- function(sum_char_dat){list(typeof(sum_char_dat), length(sum_char_dat), length(unique(sum_char_dat)))}
app_char_dat <- sapply(sum_char_dat, fun_dat)
row.names(app_char_dat) <- c("Data Type", "Length", "Number of Unique Values")
datatable(app_char_dat)
```

This table lets us see a summary of the numeric variables for the datafiniti dataset. We summarized this dataset by variable, number of values, number of zero values, number of missing values, minimum, maximum, range, sum, median, mean, standard error mean, confidence interval mean, variance, standard deviation, and coefficient variance. Also, we wanted to summarize the numeric variables through a histogram. We didn't include the longitude, latitude, and review_stats_dave_count. We didn't include the longitude and latitude, because there is too much variablity with these variables. See below for the table and histogram. Select the appropriate variable you would like to evaluate through a histogram. 

```{r}
#Creates summary data table for the numeric variables for datafiniti#
sum_num_data <- stat.desc(datafiniti_dup_removed[ , -c(1:5)])
datatable(round(sum_num_data, 2))
```

```{r}
#Creates a new dataset for the items that we want evaluated by a histogram#
datafiniti_for_histogram <- datafiniti_dup_removed[ , c(8:9)]
#Creates the interactive histogram#
shinyApp(
  ui = fluidPage(
    varSelectInput("variable", "Variable:", datafiniti_for_histogram),
    plotOutput("data" , height = 300, width = 700)
),
server = function(input, output) {
  output$data <- renderPlot({
    ggplot(datafiniti_for_histogram, aes(!!input$variable)) + geom_histogram()})
}
)
```

#### **Summary of Jared Variables**
Now, we are going to evaluate the summary of the jared variables by character and numeric variables. The first table lets us see the summary of the character variables. We summarized the character variables by the variable name, data type, length, and number of unqiue variables. See below for the table.

```{r}
#Creates summary data table of the character variables for datafiniti#
sum_char_jar <- jared2[ , c(1,2)]
fun_jar <- function(sum_char_jar){list(typeof(sum_char_jar), length(sum_char_jar), length(unique(sum_char_jar)))}
app_char_jar <- sapply(sum_char_jar, fun_jar)
row.names(app_char_jar) <- c("Data Type", "Length", "Number of Unique Values")
datatable(app_char_jar)
```

This next table lets us see a summary of the numeric variables for the jared dataset. We summarized this dataset by variable, number of values, number of zero values, number of missing values, minimum, maximum, range, sum, median, mean, standard error mean, confidence interval mean, variance, standard deviation, and coefficient variance. Also, we wanted to summarize the numeric variables through a histogram. See below for the table and histogram. Select the appropriate variable you would like to evaluate through a histogram. 

```{r}
#Creates summary data table for the numeric variables for jared#
sum_num_jar <- stat.desc(jared2[ , -c(1:2)])
datatable(round(sum_num_jar, 2))
```

```{r}
#Creates a new dataset for the items that we want evaluated by a histogram#
jared_for_histogram <- jared2[ , c(3:15)]
#Creates the interactive histogram#
shinyApp(
  ui = fluidPage(
    varSelectInput("variable", "Variable:", jared_for_histogram),
    plotOutput("data" , height = 300, width = 700)
),
server = function(input, output) {
  output$data <- renderPlot({
    ggplot(jared_for_histogram, aes(!!input$variable)) + geom_histogram()})
}
)
```

### Barstool Exploratory Data Analysis

#### **Barstool Exploratory Data Analysis Results**

The goal of this exploratory data analysis is to allow users to find the best pizza place for them based on zip code, price level, and type of rating. Another goal is to allow users to see where the top rated pizza places are on a map based on the type of rating selected. Also, a goal of ours is to allow users to see which zip codes has the highest and lowest average rating score based on the type of rating selected. Finally, we are hoping to show users which pizza place received the highest rating based on the average of all the rankings. 

This first part allows users to find the best pizza place for them based on zip code, price, level, and type of rating. In this part, we first needed to edit the final dataset for barstool in order to make visualizations for it. The first new dataset we created from the barstool dataset was to help us create a shiny table and histogram. A user should be able to select the zip, price level, and type of rating that they would like to evaluate for barstool. After selecting the results, the user would be able to see a data table and histogram of these results. The table will show the pizza place name, address, zip, price level, type of rating, and rating score. The table will be sorted in descending order by rating_score. Below is the code to create the dataset. 

```{r}
#Creates final dataset for barstool shiny table and histogram#
barstool_for_table <-  barstool_variable_name %>%
  dplyr::select("name", "address", "zip", "longitude", "latitude", "price_level", "provider_rating", "review_stats_all_average_score", "review_stats_community_average_score", "review_stats_critic_average_score", "review_stats_dave_average_score") %>% 
  gather(type_of_rating, rating_score, 7:11) %>% 
  arrange(desc(rating_score)) 
barstool_tab <- barstool_for_table[ , c(1:3,6:8)]
#Allows us to see a snippet of the final dataset for the shiny data table and histogram#
datatable(head(barstool_tab))
```

The code below creates the shiny table and histogram. Please make the selections below in order to see the data table and histogram. You can select multiple zip codes and price levels. 

```{r}
#Creates shiny table and histogram#
shinyApp(
  ui = fluidPage(selectInput("zip", "Select the zip you want to evaluate:", choices = sort(unique(barstool_tab$zip)), multiple = TRUE),
                 selectInput("price_level", "Select the price level you want to evaluate:", choices =  sort(unique(barstool_tab$price_level)), multiple = TRUE),
                 selectInput("type_of_rating", "Select the type of rating you want to evaluate:", choices = sort(unique(barstool_tab$type_of_rating)), multiple = FALSE), DT::dataTableOutput('tbl'), 
                 plotOutput("hist" , height = 300, width = 700)),
  server = function(input, output) {selected_data_tab <- reactive({
    req(input$zip)
    req(input$price_level)
    req(input$type_of_rating)
    barstool_tab %>% 
      dplyr::filter(type_of_rating %in% input$type_of_rating & zip %in% input$zip & price_level %in% input$price_level)})
    output$tbl = DT::renderDataTable(
      selected_data_tab(), options = list(lengthChange = FALSE), caption = "This table shows the pizza places based on the selections above in order of highest rating score.")
    output$hist <- renderPlot({
    ggplot(selected_data_tab(), aes(rating_score)) + 
        geom_histogram() + 
        scale_x_continuous(name = "rating score") +
        ggtitle("Histogram of Barstool Items Selected", subtitle = "Select the zip, price level, and type of rating you would like to evaluate through a histogram")})},
  options = list(height = 1200))
```

This part of the analysis allows users to see where the top rated pizza places are on a map based on the type of rating selected. We used the barstool_for_table dataset from above to plot points on the map. We decided to use a shiny map to allow users to see where the top rate pizza places are. A user will be able to select type of rating they would like to see on the map. Below is the code to create this shiny map. 

```{r}
#Creates shiny map to allow users to see where the top rated pizza places are located#
shinyApp(ui = fluidPage(
  selectInput("type_of_rating", "Select the type of rating you want to see on the map:", choices = sort(unique(barstool_for_table$type_of_rating)), multiple = FALSE),
  plotOutput("bar", height = 400, width = 800)),
server = function(input, output) {
  selected_data <- reactive({
    req(input$type_of_rating)
    barstool_for_table %>% dplyr::filter(type_of_rating %in%
                                           input$type_of_rating)})
  output$bar <- renderPlot({
    if (require("maps")) {
      data(us.cities)
      capitals <- subset(us.cities, capital == 2)
      ggplot(capitals, aes(long, lat)) +
        borders("state") + 
        geom_point(data= selected_data(), aes(x = (longitude), y = latitude, colour = rating_score)) +
        ggtitle("Barstool Map Showing the Rating Score by Rating Type", subtitle = "Select the type of rating you want to evaluate") + 
        scale_x_continuous(name = "longitude")+ scale_y_continuous(name = "latitude") + 
        coord_cartesian(xlim=c(-130, -60), ylim = c(20, 50))}})}, 
options = list(height = 500))
```

In this part, our goal is to allow users to see which zip code has the highest average rating score based on the type of rating selected. We decided that we thought we would achieve this goal through a shiny table. The first item we needed to do before we created the table was to create a dataset for it. The code below allows us to calculate the average rating score by zip and type of rating. Also, we arranged the average rating score average rating score. We thought this dataset would help us in creating the table to show which zips had the highest average rating based on type of rating selected. Below is the code to create this dataset. 

```{r}
#Creates dataset to help us create a shiny table to show which zips had the highest average rating based on the type of rating selected#
barstool_zip_1 <- barstool_for_table %>%
  group_by(zip, type_of_rating) %>%
  summarise(average_rating_score = mean(rating_score, na.rm = TRUE)) %>% 
  arrange(desc(average_rating_score)) 
datatable(head(barstool_zip_1))
```

The following code allows us to create the shiny table that was mentioned prior. Please make sure to select the type of rating you want to evaluate before you start evaluating the table. 

```{r}
#Creates shiny table showing which zips had the highest average rating based on type of rating selected#
shinyApp(
  ui = fluidPage(selectInput("type_of_rating", "Select the type of rating you want to evaluate:", choices =  sort(unique(barstool_zip_1$type_of_rating)), multiple = FALSE),
                 DT::dataTableOutput('tbl')),
  server = function(input, output) {selected_data_tab_6 <- reactive({
    req(input$type_of_rating)
    barstool_zip_1 %>% dplyr::filter(type_of_rating %in% input$type_of_rating)})
    output$tbl = DT::renderDataTable(
      selected_data_tab_6(), options = list(lengthChange = FALSE), caption = "This shows the average rating score by zip and type of rating sorted by the highest average rating score. Select the rating you want to evaluate above.")},
  options = list(height = 700))
```

The final part we wanted to do for our exploratory analysis was to allow users to see which pizza place had the highest average rating across all the type of ratings. The code below allows us to achieve this goal. In this code, we first need to group the data by name, address, and zip. Next, we created a calculation to find the mean of all the ratings. Finally, we sorted the data in descending order by average score. The last piece of code allows us to see the top 10 results for pizza places in data table. 

```{r}
#Creates dataset and data table to see the top 10 pizza places on average for all the ratings#
barstool_rate <- barstool_for_table %>%
  group_by(name, address, zip) %>%
  summarise(average_score = mean(rating_score, na.rm = TRUE)) %>% 
  arrange(desc(average_score))
datatable(head(barstool_rate, 10))
```

#### **Summary of Barstool Exploratory Analysis Results**

### Datafiniti Exploratory Data Analysis

#### **Datafiniti Exploratory Data Analysis Results**

The goal of this exploratory data analysis is to allow users to find the best pizza place for them based on province, price type, and price_range_value. Another goal of ours is to allow users to see where the cheapest pizza places are located on a map based on the price type they select. Also, we have a goal of showing users the average price value of pizza by state and price type. Finally, we have a goal of showing users which pizza places have the lowest average price value based on all the price type values. 

This first part allows users to find cheapest pizza place for them. They are able to sele

```{r}
#Creates data set for our shiny table and histogram#
datafiniti_map <- datafiniti_dup_removed %>%
                      gather(price_type, price_range_value, 8:9) %>%
                      arrange(price_range_value)
datafiniti_tab <- datafiniti_map[ , c(1:3,5,8:9)]
datatable(head(datafiniti_tab))
```

```{r}

shinyApp(
  ui = fluidPage(selectInput("province", "Select the state you want to evaluate:", choices = sort(unique(datafiniti_tab$province)), multiple = TRUE),
                 selectInput("price_type", "Select the price type you want to evaluate:", choices =  sort(unique(datafiniti_tab$price_type)), multiple = TRUE),
                 selectInput("price_range_value", "Select the price range value you want to evaluate:", choices =  sort(unique(datafiniti_tab$price_range_value)), multiple = TRUE),
                DT::dataTableOutput('tbl'),
  plotOutput("data_2" , height = 300, width = 700)),
  server = function(input, output) {selected_data_tab_2 <- reactive({
    req(input$province)
    req(input$price_type)
    req(input$price_range_value)
    datafiniti_tab %>% dplyr::filter(province %in% input$province & price_type %in% input$price_type)})
    output$tbl = DT::renderDataTable(
      selected_data_tab_2(), options = list(lengthChange = FALSE), caption = "This table shows the pizza places based on the selections above in order based on the lowest price range value.")
    output$data_2 <- renderPlot({
    ggplot(selected_data_tab_2(), aes(price_range_value)) + geom_histogram() + scale_x_continuous(name = "price range value") + ggtitle("Histogram of Datafiniti Items Selected", subtitle = "Select the state, price type, and price range value you want to evaluate")
  }
)
  },
  
  options = list(height = 1000)
)
```

```{r}
datafiniti_state <- datafiniti_map %>%
                      group_by(province, price_type) %>%
                      summarise(average_price_value = mean(price_range_value, na.rm = TRUE)) %>% arrange(average_price_value) 
```

```{r}
datafiniti_price <- datafiniti_map %>%
  group_by(name, address, city) %>%
  summarise(average_price = mean(price_range_value, na.rm = TRUE)) %>%
  arrange(average_price)
datatable(datafiniti_price)
```

```{r}
datafiniti_state_2 <- datafiniti_state %>% 
  arrange(desc(average_price_value))
```




```{r}
data_combine <- barstool_for_table %>%
  inner_join(datafiniti_map, by = "address") %>%
  dplyr::select("name.x", "address", "zip", "longitude.x", "latitude.x", "type_of_rating", "rating_score", "price_type", "price_range_value") %>%
  arrange(desc(rating_score))

datatable(data_combine)
```

### Shiny Table







```{r}
shinyApp(ui = fluidPage(
  selectInput("price_type", "Select the price type you want to see on the map:", choices = sort(unique(datafiniti_map$price_type)), multiple = FALSE),
  plotOutput("bar", height = 400, width = 800)
),
server = function(input, output) {
  selected_data_2 <- reactive({
    req(input$price_type)
    datafiniti_map %>% dplyr::filter(price_type %in% input$price_type)})
  output$bar <- renderPlot({
 if (require("maps")) {
  data(us.cities)
  capitals <- subset(us.cities, capital == 2)
  ggplot(capitals, aes(long, lat)) +
    borders("state") + 
    geom_point(data= selected_data_2(), aes(x = (longitude), y = latitude, colour = price_range_value)) +
    ggtitle("Datfiniti Map Showing the Price Range Value by Price Type", subtitle = "Select the price type you want to evaluate") + 
    scale_x_continuous(name = "longitude") + scale_y_continuous(name = "latitude") +
    coord_cartesian(xlim=c(-130, -60), ylim = c(20, 50))}})
}, 
options = list(height = 500))
```



```{r}
shinyApp(
  ui = fluidPage(selectInput("price_type", "Select the price type you want to evaluate:", choices =  sort(unique(datafiniti_state$price_type)), multiple = FALSE),
                DT::dataTableOutput('tbl')),
  server = function(input, output) {selected_data_tab_3 <- reactive({
    req(input$price_type)
    datafiniti_state %>% dplyr::filter(price_type %in% input$price_type)})
    output$tbl = DT::renderDataTable(
      selected_data_tab_3(), options = list(lengthChange = FALSE), caption = "This shows the average price value by state and price_type. This is sorted by the lowest average price value. Select the price type you want to evaluate.")},
  
  options = list(height = 700))
```

```{r}
data_tab <- data_combine[, c(1:3, 6:9)]
shinyApp(
  ui = fluidPage(selectInput("zip", "Select the zip you want to evaluate:", choices = sort(unique(data_tab$zip)), multiple = TRUE),
                 selectInput("price_range_value", "Select the price range value you want to evaluate:", choices =  sort(unique(data_tab$price_range_value)), multiple = TRUE),
                 selectInput("type_of_rating", "Select the type of rating you want to evaluate:", choices = sort(unique(data_tab$type_of_rating)), multiple = FALSE), DT::dataTableOutput('tbl'), plotOutput("hist_4" , height = 300, width = 700)),
  server = function(input, output) {selected_data_tab_6 <- reactive({
    req(input$zip)
    req(input$price_range_value)
    req(input$type_of_rating)
    data_tab %>% dplyr::filter(type_of_rating %in% input$type_of_rating & zip %in% input$zip & price_range_value %in% input$price_range_value)})
    output$tbl = DT::renderDataTable(
      selected_data_tab_6(), options = list(lengthChange = FALSE), caption = "This table shows the pizza place with the highest rating score based on the selections above.")
    output$hist_4 <- renderPlot({
    ggplot(selected_data_tab_6(), aes(rating_score)) + geom_histogram() + ggtitle("Histogram of Datafiniti and Barstool Combined Rating Score", subtitle = "Select the zip, price range value, and type of rating you want to evaluate") + scale_x_continuous(name = "rating score")})
  },
  
  options = list(height = 1200, width = 900))
```

```{r}
jared_tab_good <- jared2 %>%
  mutate(bad_rating = Pct_Never_Again + Pct_Poor) %>%
  mutate(good_rating = Pct_Good + Pct_Excellent) 
jared_new_calc <- jared_tab_good[ ,c(1, 16:17)]
jared_good <- jared_new_calc %>%
  arrange(desc(good_rating))
jared_bad <- jared_new_calc %>%
  arrange(desc(bad_rating))
datatable(head(jared_good, 10), caption = "This table shows the top 10 jared items that have a good rating.")
datatable(head(jared_bad, 10), caption = "This table shows the top 10 jared items that have a bad rating.")

```